# Reproducible, Reusable, and Robust Reinforcement Learning

Slide can be downloaded from [here](https://media.neurips.cc/Conferences/NIPS2018/Slides/jpineau-NeurIPS-dec18-fb.pdf)

Recording can be found [here](https://www.youtube.com/watch?v=Kee4ch3miVA)

We have seen significant achievements with deep reinforcement learning in
recent years. Yet reproducing results for state-of-the-art deep RL methods is
seldom straightforward. High variance of some methods can make learning
particularly difficult when environments or rewards are strongly stochastic.
Furthermore, results can be brittle to even minor perturbations in the domain
or experimental procedure. In this talk, I will review challenges that arise in
experimental techniques and reporting procedures in deep RL. I will also
describe several recent results and guidelines designed to make future results
more reproducible, reusable and robust.

## Key Points

* The reinforcement learning community has a reproducibility crisis
* Reproducibility Checklist
* Generalization problem on RL itself. How to avoid testing beyond the training
  data
* ICLR 2018 Reproducibility Challenges

Common RL test datasets:

* DDPG
* TRPO
* PPO

## References:

[An Introduction to Deep Reinforcement Learning](https://arxiv.org/pdf/1811.12560.pdf)